mr - used Python MapReduce to analyze the Wikipedia dataset 

    • Parsed all words in simple Wikipedia [regex, stringmethods, lxml, beautifulsoup, mwparserfromhell]
    • Found 100 most frequent words [heapq]
        ◦ Create word cloud
    • Calculated Shannon entropy of n-grams for n=1,2,3 for English and Thai Wikipedia corpora
    • Calculated number of links per page on simple and full English Wikipedia
    • Found avg, std, and quantiles 1-4 of links per page [reservoir sampling]